$margin 10 $width 75
$spacing 2
$title 'AN OPTIMIZING PASCAL COMPILER'
$center
R. Neil Faiman, Jr.   and   Alan A. Kortesoja
$title ''
$skip 2 $center
ABSTRACT
$skip $just $para 5
The architecture of a production optimizing compiler for Pascal is described,
and the structure of the optimizer is detailed.
The compiler performs both interprocedural and global optimizations, in addition
to optimization of basic blocks.
We have found that a high-level structured language such as Pascal provides
unique opportunities for effective optimization, but that standard
optimization techniques must be extended to take advantage of these
opportunities.
These issues are considered in our discussion of the optimization algorithms
we have developed and the sequence in which we apply them.
$skip 2 $need 5 $center
I.   INTRODUCTION
$skip $just $para 5
It is commonly assumed that it ought to be easy to generate efficient code
for structured programs written in a structured language.
Wirth proposed the development of implementations "which are both
reliable and efficient on presently available computers" as a principal
aim of the language Pascal [15].
However, the very advantages of high-level, structured languages such as Pascal
present unique challenges to the compiler writer.
For example, the structure of Pascal programs introduces opportunities for
interprocedural optimizations which are not available in compiling a language
such as Fortran.
Similarly, techniques can be devised to improve the optimization of
programs which use structured data.
$skip $need 3 $para 5
We have designed and implemented a production optimizing compiler for Pascal
on the DECsystem-10.
Our compiler incorporates algorithms from a variety of sources.
We have adapted these algorithms to optimize Pascal more effectively,
and have modified them to work together.
This paper discusses several of these algorithms as they are used in our
compiler.
$skip $need 3 $para 5
Our compiler accepts the full Pascal language,
including features posing particular problems for optimization, such
as goto statements out of procedures, and
procedural and functional parameters.
Our language also provides certain useful extensions [17].
For example, we allow separate compilation of "modules," which may
share procedures, functions, and variables;
and we permit functions to return values of any type.
All of these standard and extended language features are accomodated by our
optimization algorithms.
$skip $need 3 $para 5
We have found that seemingly minor language changes can result in
disproportionate
improvements in the use of the language and the effectiveness of the
optimization algorithms.
For example,
in standard Pascal, call-by-value parameters must always be copied since they may be modified.
The overhead of doing this is such that the Pascal User Manual [15]
suggests that it may be better to pass certain structured variables as
variable parameters, even when they are not modified by the called procedure.
This subverts the distinction between value and variable parameters,
and makes it unreasonable for a compiler to assume that variable parameters are modified by
their procedures and value parameters are not.
An optimizing compiler must therefore perform extensive analysis to determine
whether a variable parameter is really modified.
In our implementation, we
forbid assignment to a call-by-value parameter within a procedure or function.
This allows us to pass the address instead of the value of an array or record
parameter.
Thus, there is no efficiency penalty for the use of value parameters;
programmers use value parameters when they want a value and variable parameters
when they want a reference;
and the optimizer can reasonably assume that variable parameters are modified
by their procedures.
$skip $need 3 $para 5
The structure of our compiler differs from that of most Pascal compilers,
which have fundamentally the same one-pass, recursive descent structure as
Wirth's CDC Pascal compiler [22].
The program representation used in our optimizer is novel,
as it assumes two different forms during optimization.
These forms are not related to the PCODE representation produced by the
Portable Pascal compiler [4], on which other multi-pass compilers
have been based [18].
Our original intermediate form is machine-independent, and manipulates data
represented as records, arrays, sets, etc.
This is converted to a low-level form which more closely models the actual
memory accesses and computations required on the target machine.
Our intermediate form allows a great deal of flexibility for optimization;
we apply optimizing transformations to both the original and low-level
intermediate forms.
$skip $need 3 $para 5
The code generated by our compiler is typically 30-40% smaller than that
produced by the DECsystem-10 compiler of Nagel &et al&.\[13],
which is representative of many one-pass Pascal compilers.
(These results are discussed further in section VII.)\
It should be noted that this reduction was achieved without attempting to
recognize many special-case optimization
opportunities (e.g., assignments of the form "X\:=\X\op\Y"),
and with a primitive register allocation algorithm.
We intend to add such improvements in the future.
$skip $need 3 $para 5
Our compiler runs in three passes, as shown in figure 1.
The parser reads the program source code and produces a
symbol table and a high-level intermediate form representation.
The optimizer transforms the initial, high-level intermediate form (HLIF)
into optimized low-level intermediate form (LLIF).
The code generator converts the low-level intermediate form into machine code.
In this paper, we consider only the optimizing pass of the compiler.
$skip 2 $need 6 $center
II.   OPTIMIZER ARCHITECTURE
$skip $just $para 5
Both the high-level and the low-level intermediate forms are composed of
n-tuples,
which are divided into statement tuples and expression tuples.
All tuples have explicit pointers to the expression tuples representing their
operands,
so expressions may be processed recursively as tree structures.
In addition, all the tuples in the intermediate form are linked together in
a linear list, in which a tuple follows all of its operands,
so the intermediate form may be processed in a simple linear scan.
This hybrid form simplifies the recovery of tuples which become unreferenced
during optimization
and the computation of usage counts for register allocation [11].
$skip $need 3 $para 5
The structure of the optimizer is shown in figure 2.
The optimization pass begins with application of the interprocedural analysis
algorithm (section III).
It then applies the following steps to each procedure in the program (including
the main program itself):
$indent +5 $skip $need 3 $para -5
(a)\\Flow graph analysis.
Basic blocks are recognized, jumps to jumps are eliminated, and boolean
operators are converted to branch logic.
If there are procedure or function calls which might result in non-local
gotos to labels in the procedure, special flow graph edges are introduced
from the calls to the labels, using information obtained during interprocedural
analysis.
The basic blocks of the intermediate form are reordered, using the depth-first
spanning tree (DFST) ordering on the flow graph [2],
which allows easy detection of loops and improves the efficiency of a number
of the algorithms which are applied later.
$skip $need 3 $para -5
(b)\\High-level optimization (sections IV and V).
The basic blocks of the program are reduced by elimination of common
subexpressions, and global
(intraprocedural) optimizations are applied.
$skip $need 3 $para -5
(c)\\Shaping.
This step converts the high-level intermediate form into the low-level
intermediate form.
The HLIF and LLIF use the same statement tuples, and most of the same
expression operator tuples.
They differ in their treatment of data references.
In the HLIF, there are tuples for references to constants, variable identifiers,
array elements, fields of records, etc.
In the LLIF, these are replaced by tuples representing immediate values, memory
addresses, and the contents of memory words in terms of constant or symbolic
offsets and computed base and index operands.
The general form of the LLIF is machine-independent, but the transformations
in this step may be tailored to the addressing structure of the target machine.
The LLIF is simplified by application of simple algebraic rules.
For example, the constant part of a subscript calculation may be replaced
by a constant offset in an indexed memory reference.
$skip $need 3 $para -5
(d)\\Low-level optimization (section VI).
The shaping step frequently introduces redundant computations, particularly
in the code generated to compute the addresses of array elements.
These redundant computations are eliminated by the low-level optimization step.
$indent -5 $skip $need 3 $para 5
Figure 3(a) shows a fragment of Pascal source code.
The high-level intermediate form into which this is translated by the analysis
pass is given in figure 3(b), while the optimized, low-level intermediate form
produced by the optimization pass is given in figure 3(c).
$skip $need 3 $para 5
In this paper, we will only consider the algorithms used in interprocedural
analysis and in high-level and low-level optimization,
since the flow graph analysis is largely conventional, and the shaping step
is more closely related to code generation than to optimization.
$skip 2 $need 6 $center
III.   INTERPROCEDURAL ANALYSIS
$skip $just $para 5
Interprocedural analysis actually comprises two distinct tasks,
quick block analysis and summary data flow analysis.
$skip $need 3 $para 5
Two language features make interprocedural analysis particularly interesting.
The first is the ability to define a routine (i.e., a procedure or function)
or a variable as PUBLIC, meaning
that it may be referred to in separately compiled programs, or as EXTERNAL,
meaning that its actual definition is in a separately compiled program.
The second is the ability to specify procedural and functional parameters and
variables.
We require that all such parameters and
variables be declared with a "routine type,"
which fully specifies their parameterization.
$skip $need 3 $para 5
Interprocedural analysis makes use of the program call graph.
The call graph contains nodes for the main program, all external routines,
all routine variables, each defined routine, and each procedural or
functional parameter.
Edges in the call graph represent calling and binding relationships.
We assume that a call on a routine with a procedural or functional parameter
will result in a call on its parameter (an assumption which is both conservative
and reasonable), and define the edges in the call graph to reflect this
assumption.
$skip 2 $need 6
A.\\\&Quick Block Analysis&
$skip $para 5
The performance of a highly modularized, block structured program is strongly
affected by the procedure linkage protocol.
The need to initialize stack frames and to maintain a static chain or display
imposes an overhead on all routine calls.
Quick block analysis determines when a routine is non-recursive and can be
called (directly or indirectly) only from some other routine.
Such a routine (a "quick routine") can have its storage allocated in
the same area of the stack and at the same time as the routine from which it is always called
(its "owner").
This has several advantages.
$skip $need 3 $para 5
First, since the quick routine does not need a separate stack area of its own,
calls to it do not incur the overhead associated with initializing a stack
area.
This saving can be significant:
in our implementation, the call, entry and return code for a quick routine
requires only two instructions, compared with nine instructions for a non-quick
routine.
$skip $need 3 $para 5
Another saving occurs when one routine is declared within another.
An inner routine can refer to variables declared in the outer routine.
In a conventional implementation, such global references require
indirection via a static chain, a display, or some other mechanism.
However, if the inner routine is a quick routine (as it frequently is),
then it will use the same stack area as the outer routine, and can
access the outer routine's variables the same way it accesses its own.
$skip $need 3 $para 5
Quick block analysis also improves the performance of the optimizer itself.
The execution time of our summary data flow analysis algorithm (section III.B)
is proportional to the maximum depth of nesting of procedure and function
declarations in the program.
By effectively reducing this depth, quick block analysis makes summary data
flow analysis run faster.
$skip $need 3 $para 5
Recognition of quick routines is based on the following observations:
$indent +5 $skip $need 3 $para -5
(1)\\The main program and any public routines are non-quick by definition,
since they are entry points of the entire program, and must have their own
stack areas.
$skip $need 3 $para -5
(2)\\A recursive routine is non-quick, since it must create a new stack area
for each version of its local variables.
$skip $need 3 $para -5
(3)\\A routine which is called by two other routines with different owners
is non-quick, since it must have a unique owner.
$skip $need 3 $para -5
(4)\\A routine which is passed as a parameter or assigned to a variable
is non-quick, since the set of its possible callers is unknown.
$skip $need 3 $para -5
(5)\\If a routine does not fall into any of the above categories, then all the
routines which call it have the same owner, so it too may share the stack
area of that common owner.
$indent -5 $skip $need 3
A non-quick routine owns itself.
A quick routine is owned by the common owner of all its callers.
$skip $need 3 $para 5
We have developed an algorithm for determining the owners of all
routines in the program, and for overlaying the local storage for routines
sharing the same stack area.
Our algorithm is based on depth-first search [20], and therefore its time
and space requirements are linear in the number of routines and calling
relations in the call graph.
Since the call graph is usually quite sparse, this can be considerably more
efficient than the algorithm of Walter [21], which
represents the calling relations with bit matrices, and therefore requires
time and space proportional to the square of the number of routines in the
call graph.
$skip 2 $need 6
B.\\\&Summary Data Flow Analysis&
$skip $para 5
Summary data flow analysis determines which variables may be used and/or
modified by a routine call.
This information is needed to deal properly with calls during
subsequent optimization.
$skip $need 3 $para 5
Our algorithm for summary data flow analysis [10] is derived from algorithms
due to Barth [5,6] and Hecht [14].
Using cross-reference information saved by the parser, we compute, for each
routine P, the sets DIRECTMOD(P) and DIRECTUSE(P).
DIRECTMOD(P) is the set of variables which are modified by P itself, and
DIRECTUSE(P) is the set of variables whose values are used in P.
Using these sets and the call graph,
the algorithm computes MOD(P) and USE(P), the sets of all the variables which
may be modified or used as a consequence of a call on P.
$skip $need 3 $para 5
For purposes of our algorithm, the set of all variables includes all declared
variables and variable parameter symbols in the program.
For each pointer type, all the dynamically allocated variables which can be
accessed with pointers of that type are treated as a single variable.
We also treat declared labels as variables, including a label
symbol in DIRECTMOD(P) if P contains a goto statement
to that symbol.
This artifice allows us to obtain the information about non-local goto
statements which is needed for flow graph analysis.
$skip $need 3 $para 5
We make two important simplifying assumptions in our algorithm.
First, we assume that a variable parameter of a routine is modified by the
routine.
(The justification for this assumption is discussed in section I.)
$skip $need 3 $para 5
We also specify that if a routine accesses the same
variable by more than one name, i.e., if two parameters refer to the same
actual variable, or if the routine makes a global reference to a variable
which is passed to it as a parameter, the results are undefined.
In conjunction with the first assumption above, this obviates the need for
any sort of aliasing analysis on parameters.
This simplifies the algorithm considerably.
Furthermore, if a procedure has a variable parameter which it modifies,
Barth's algorithm will indicate that any call on this procedure will
modify all variables which are ever bound to the parameter.
Our algorithm, in contrast, will only indicate a modification of the variable
actually passed in each call to the procedure.
$skip $need 3 $para 5
Let N be the number of procedures in the call graph, and E the number of edges.
The algorithm described by Barth uses bit-matrix representations of the CALLS,
DIRECTMOD, and DIRECTUSE relations, and computes the MOD and USE relations
with bit-matrix operations.
Its run time is therefore at least of order N^1a2.
Hecht's algorithm is based on Tarjan's depth-first spanning tree algorithm
for strongly connected components [3,20], and therefore has run time on the order of N+E,
but it was designed for SIMPL-T, a non-block structured language in which
scope considerations do not arise.
By applying Tarjan's algorithm to nested subgraphs of the call graph at
successive levels of scope,
our algorithm computes MOD and USE as precisely as possible
under the assumption that all statements in a procedure are executable.
Since the quick block analysis described above reduces the effective depth
of nesting, the worst-case run time for our algorithm is on the order of (N+E)*L,
where L is the maximum effective nesting level of routine definitions.
This is typically less than the true maximum depth of nesting.
$skip 2 $need 6 $center
IV.   BASIC BLOCK REDUCTION
$skip $just $para 5
Common subexpre elimination within a basic block is well understood.
A number of algorithms have been published (for example, [1,2,12,14]), most of
them based to some extent on the value number method of Cocke and Schwartz [7].
While these algorithms will produce optimal results in programs containing only
simple variables, their treatment of structured variables is limited.
In general, they consider only array variables, and treat an assignment to any
component of an array as a redefinition of the entire array.
$skip $need 3 $para 5
In modern programming languages such as Pascal, structured variables
are used extensively.
Therefore, a prime objective in the design of our optimization algorithm was
that it should give good results in the presence of structured variables, and
that where possible it should take advantage of information from declarations
to give better results than would be possible with unstructured data.
$skip $need 3 $para 5
On the other hand, optimal algorithms for programs with structured variables
can be very expensive.
For example, it is shown in [8] that the equivalence problem for
straight line programs with array assignments is NP-Hard.
Therefore, we have developed a heuristic algorithm [9] based on the Cocke and
Schwartz common subexpression elimination algorithm.
$skip $need 3 $para 5
Like the Cocke and Schwartz algorithm, ours keeps a value for each symbolic
variable and a hash table of current expressions, each with a unique value
number.
However, we update the value number of a structured variable (i.e., an
array or record) only when an assignment is made to the entire variable.
We also keep an auxiliary table of statements which modify variables;
when we find a common subexpression which is a component of a
structured variable, we
check the table to see whether any statement has been processed which could
invalidate this expression.
$skip $need 3 $para 5
Our algorithm is more expensive than the Cocke and Schwartz algorithm,
since it requires a check of the modifying statements table whenever
a common component reference is found.
Our return comes in the latitude which we have in
deciding whether a statement invalidates a reference.
This test may be as simple or as complex as the compiler writer desires;
it may check for special cases where that seems appropriate;
and it may be modified to allow for different data types.
In our implementation, we use a simple recursive test function.
$skip $need 3 $para 5
Consider the program fragment shown in figure 4.
Our common subexpression algorithm will conclude that, in the last statement,
"p^.c1" must indeed be equal to "10".
This conclusion is valid because it is illegal in Pascal to refer to a field of a record
which is in a variant other than the one selected by the current value of the
record's tag field.
Thus, the assignment to "q^.c2" could interfere with the value of "p^.c1" only
if "p" and "q" pointed to the same record;
but if "p" and "q" pointed to the same record, then one of the two assignments
would be illegal, and the program would be erroneous to begin with.
$skip $need 3 $para 5
In general, we have not hesitated to assume that the programs processed by
our compiler will satisfy all language restrictions.
Therefore, we will allow optimized and unoptimized versions of a program
to yield different results, if our language specification [17] indicates that
those results are "undefined."
If a programmer truly needs to evade some language restriction which causes
problems only when optimization is in effect (such as the variant record access
restriction above), then optimization can be suppressed for the compilation of
a particular routine.
$skip $need 3 $para 5
The reduction algorithm can also detect redundant assignments.
An assignment whose left and right sides have the same values is simply
eliminated.
An assignment whose left side is later reassigned is eliminated, but the
right side is still evaluated if it could have side effects.
$skip 2 $need 6 $center
V.   GLOBAL OPTIMIZATION
$skip $just $para 5
One block in the flow graph of a program is said to dominate another if all
control flow paths from the start of the program to the second block must
pass through the first block.
The heart of our global optimization is the observation that global common
subexpression elimination may be performed by the same algorithm as local
common subexpression elimination, by applying the algorithm along paths in the
dominator tree, instead of within basic blocks.
$skip $need 3 $para 5
The dominator tree can be computed efficiently with an algorithm by
Lengauer and Tarjan [16].
Using an algorithm by Reif [19], we can then find the sets of variables which
can be modified along each control flow path from a dominating block to a
dominated block, and thence the earliest dominating block from which the
value of each reference is unchanged.
Finally, we use this information to reapply the basic block reduction algorithm,
reducing entire dominator chains instead of basic blocks.
$skip $need 3 $para 5
$bottom 5
Since the algorithms used in this process are bit-vector
algorithms, we need a mapping from the intermediate form expressions to the
bit-vector indices which
minimizes the size of the bit-vectors without losing too
much useful information about the data flow in the program.
Rather than using the actual variables of the program, we introduce a set of
"Formal Reference Expressions", or FREs.
These FREs are defined recursively:
$skip $need 4 $ind +5
$para -3
o\\There is a unique FRE for each symbolic variable.
$skip $para -3
o\\For each FRE representing an array, there is an FRE representing all of the
elements of the array.
$skip $para -3
o\\For each pointer type, there is an FRE representing all the dynamically
allocated variables to which pointer variables of that type might point.
$skip $para -3
o\\For each FRE representing a record, there is a unique FRE for each field
of that record.
$ind -5 $skip 2 $need 6 $center
VI.   LOW-LEVEL OPTIMIZATION
$skip $just $para 5
Like the high-level global optimization algorithm, low-level optimization
finds common subexpressions along dominator chains of the flow graph.
However, this algorithm makes no attempt to optimize assignments or
memory references, since optimizations involving the values and lifetimes of
variables are applied during high-level optimization.
Low-level optimization is only concerned with new expressions
which may be introduced during shaping.
Consequently, the low-level optimization algorithm is a very simplified
version of the algorithm that is used for basic block and global reduction
during high-level optimization.
$skip 2 $need 6 $center
VII.  PRACTICAL EXPERIENCE
$skip $just $para 5
In developing our optimization algorithms, we have concentrated on reducing
generated code size.
Code size is our first concern because our compiler will ultimately be used
in the implementation of large systems on small computers.
Furthermore, the elimination of instructions will generally result in
a corresponding improvement in execution speed.
$skip $need 3 $para 5
We have compared the code produced by our new compiler with that produced by
the Pascal compiler we used previously.
(This was based on the DECsystem-10 compiler described in [13], with many of
the same language extensions that we have since incorporated in the new
compiler.)\
Some sample code size improvements are:
40% smaller on a quick-sort procedure, 27% on a program to solve the Dutch national
flag problem, 58% on a program which solved a heat diffusion problem with
a two-dimensional finite difference algorithm (this program was particularly
susceptible to elimination of common subscript calculations), and 36% on the
flow graph analysis routines of the new compiler itself.
$skip $need 3 $para 5
Compilation time comparisons have been less satisfactory.
Initially, compilation with the new compiler was much slower than with our
previous compiler -- frequently by a factor of four.
To some extent, this was attributable to our consistent adherence to a design
philosophy of making the compiler work first, and then making it faster.
However, it was also due to the fundamental differences between a one-pass
compiler, which transforms the source code directly into object code, and
a multi-pass compiler which makes successive transformations between
source code, abstract parse tree, high-level intermediate form, low-level
intermediate form, and object code.
$skip $need 3 $para 5
We have developed two solutions for this problem.
The first is based on the observation that, during program development,
optimization is not only unnecessary (modules may be used only once or twice
between recompilations), but undesirable, since it makes it harder to relate
any errors that occur in the program to its source structure.
Therefore, users generally suppress optimization during program development.
However, even with optimization suppressed, the compiler still had to convert
the high-level intermediate form to low-level intermediate form before it
could generate code.
Therefore, we wrote a new code generator, the checkout code generator, which
generates code directly from the high-level intermediate form (figure 5).
The code generated by the checkout code generator is generally as good as
code generated by the optimization-path code generator when optimization is
suppressed, but eliminating the conversion to the low-level intermediate
form reduces the total compilation cost considerably.
Since the checkout and optimization paths share the same analysis pass, no
special effort is needed to make sure that they compile the same language.
$skip $need 3 $para 5
Our second solution is based on the observation that modules of a
large program must contain all the constant and type definitions needed to
manipulate the data of that program.
These definitions do not impose any cost on the optimization and code
generation passes, but we found that in many cases, as much as 80% of the
analysis pass execution time could be attributed to definition processing.
Since these definitions are largely common to all the modules of a program,
our solution was to provide a compiler feature to allow the pre-compilation
of a collection of declarations.
Modules could then be compiled with these declarations in their initial
environment, just as modules are normally compiled with the standard language
identifiers (type names, built-in functions, etc.) in their initial
environment.
$skip $need 3 $para 5
With these two new features, we have been able to halve the cost
of using our new compiler.
This means that our new compiler is still twice as expensive to use as our previous
one, due largely to the cost of translating to intermediate form before
generating code.
However, in exchange for this additional cost, we have the ability to use the
same compiler to generate code for any target machine, simply by writing a new
code generator to use the same intermediate form.
For example, it took us only eight man-months to produce a checkout code generator
for the DEC VAX-11/780;
and using this new code generator, a multi-program system of approximately
85,000 lines of Pascal which had been developed on the DEC-10 was transferred
to the VAX in three weeks.
$bottom 6
$page $center
REFERENCES
$skip $just left $indent +6 $para -6
[1]\\\A.\V.\Aho and J.\D.\Ullman, "Optimization of straight line programs,"
&SIAM J.&\&Comput.&, vol.\1, pp.\1-19, March 1972.
$skip $need 3 $para -6
[2]\\\A.\V.\Aho, J.\E.\Hopcroft, and J.\D.\Ullman, &Principles of Compiler
Design&, Addison-Wesley, Reading, Mass., 1977.
$skip $need 3 $para -6
[3]\\\A.\V.\Aho and J.\D.\Ullman, &The Design and Analysis of Computer
Algorithms&, Addison-Wesley, Reading, Mass., 1974.
$skip $need 3 $para -6
[4]\\\U.\Ammann, "The method of structured programming applied to the development
of a compiler," in &Proc.&\&ACM Intern. Compiler Symp.&\&1973&, North-Holland,
New York, 1973, p.\93.
$skip $need 3 $para -6
[5]\\\J.\M.\Barth, "A practical interprocedural data-flow analysis algorithm,"
&Comm.&\&ACM&, vol.\21, pp.\724-736, Sept.\1978.
$skip $need 3 $para -6
[6]\\\J.\M.\Barth, "A practical interprocedural data flow analysis algorithm and
its applications," Ph.D.\Th., U.\of California, Berkeley, 1977.
$skip $need 3 $para -6
[7]\\\J.\Cocke and J.\T.\Schwartz, &Programming Languages and their Compilers:
Preliminary Notes&, New York University, 1970.
$skip $need 3 $para -6
[8]\\\P.\J.\Downey and R.\Sethi, "Assignment commands with array references,"
&J.&\&ACM&, vol.\25, pp.\652-666, Oct.\1978.
$skip $need 3 $para -6
[9]\\\R. N. Faiman, "Reduction of basic blocks to DAGs," &Compiler Implementation
Notes #5&, Manufacturing Data Systems, Inc. (Programming Languages Dept.),
Ann Arbor, MI, 1978.
$skip $need 3 $para -6
[10]\\R.\N.\Faiman, "Summary Data Flow Analysis," &Compiler Implementation
Notes #7&, Manufacturing Data Systems, Inc. (Programming Languages Dept.),
Ann Arbor, MI, 1979.
$skip $need 3 $para -6
[11]\\R.\A.\Freiburghouse, "Register allocation via usage counts," &Comm.&\&ACM&,
vol.\17, pp.\638-642, Nov.\1974.
$skip $need 3 $para -6
[12]\\D. Gries, &Compiler Construction for Digital Computers&, Wiley, New York,
1971, pp.\377-382.
$skip $need 3 $para -6
[13]\\C.\O.\Grosse-Lindemann and H.\H.\Nagel, "Postlude to a Pascal-compiler
bootstrap on a DECsystem-10," &Software&--&Practice and Experience&, vol.\6,
pp.\29-42, Jan.\1976.
$skip $need 3 $para -6
[14]\\M.\S.\Hecht, &Flow Analysis of Computer Programs&, North-Holland, New York,
1977.
$skip $need 3 $para -6
[15]\\K.\Jensen and N.\Wirth, &Pascal: User Manual and Report&,
Springer-Verlag, New York, 1974.
$skip $need 3 $para -6
[16]\\T.\Lengauer and R.\E.\Tarjan, "A fast algorithm for finding dominators
in a flow graph," &ACM Trans.&\&on Prog.&\&Lang.&\&and Systems&, vol.\1,
pp.\121-141, July 1979.
$skip $need 3 $para -6
[17]\\&MDSI Pascal Report&, Manufacturing Data Systems, Inc. (Programming
Languages Dept.), Ann Arbor, MI, 1979.
$skip $need 3 $para -6
[18]\\D.\R.\Perkins and R.\L.\Sites, "Machine-independent Pascal code
optimization," in &Proc.&\&of the SIGPLAN Symp.&\&on Compiler Construction&,
&ACM SIGPLAN Notices&, vol.\14, pp.\201-207, Aug.\1979.
$skip $need 3 $para -6
[19]\\J.\H.\Reif, "Combinatorial aspects of symbolic program analysis,"
Ph.D.\Th., Harvard U., 1977.
$skip $need 6 $para -6
[20]\\R.\E.\Tarjan, "Depth-first search and linear graph algorithms,"
&SIAM J.&\&Comput.&, vol.\1, pp.\146-160, June 1972.
$skip $need 3 $para -6
[21]\\K.\G.\Walter, "Recursion Analysis for Compiler Optimization," &Comm.&\&ACM&,
vol.\19, pp.\514-516, Sept.\1976.
$skip $need 3 $para -6
[22]\\N.\Wirth, "The design of a Pascal compiler," &Software&--&Practice and
Experience&, vol.\1, pp.\309-333, 1971.
$page
$number off
$verb
FOOTNOTES
$skip 3 $just
&Affiliation of authors&
$skip 2
The authors are with Manufacturing Data Systems, Inc., 4251 Plymouth Road,
P.\O.\Box 986, Ann Arbor, Michigan 48106.
$page $verb
AUTHORS' BIOGRAPHIES
$skip 3 $just
Neil Faiman received the B.S. and M.S. degrees in computer science
in 1974 and 1975 from Michigan State University, East Lansing.
$skip
He worked for Burroughs Corp. from 1975 to 1977.
Since 1977 he has been with the Programming Languages Department of
Manufacturing Data Systems, Inc., Ann Arbor, MI, where he has participated in
the development of MDSI's Pascal compiler and programming environment.
His interests are in programming language development, code optimization,
formal semantics, and software tools.
$skip
Mr.\Faiman is a member of the Association for Computing Machinery, the IEEE
Computer Society, and Tau Beta Pi.
$skip 3
Alan A.\Kortesoja is presently Acting Director of Software Technology at
MDSI.
His responsibilities include management of the Programming Languages
Department, which he has headed since joining the company in 1975.
He was a member of the technical staff at Comshare, Inc., from 1972 to
1975, and was employed by Willow Run Labs at the University of Michigan
from 1970 to 1972, and by Conductron Corp., a subsidiary of McDonnell
Douglas, in 1969 and 1970.
His interests include software tools, software engineering, and programming
languages, and he has published several papers in these fields.
$skip
Mr.\Kortesoja is a graduate of the University of Michigan in computer
science, and is a member of the Association for Computing Machinery, the
IEEE Computer Society, and the IEEE/X3J9 Pascal Standards Committee.
$page $verb
FIGURE CAPTIONS
$verb $skip 2
Figure 1.  Compiler Structure
Figure 2.  Optimizer Structure
Figure 3.  Intermediate Form Example
   -- (a)  Source Code
   -- (b)  Original High-Level Intermediate Form
   -- (c)  Optimized Low-Level Intermediate Form
Figure 4.  Pascal Program Fragment
Figure 5.  Checkout Compilation Path
$page $verb
INDEX TERMS
$skip 3 $ind +10
Compilers
Code Optimization
Programming Languages
Structured Programming
Pascal
$page $ind -10 $verb
Address for Correspondence
$skip 3 $ind +10
R. Neil Faiman, Jr.
MDSI
4251 Plymouth Road
P. O. Box 986
Ann Arbor, Michigan 48106
$skip 10 $ind -10 $just left
You have the authors' permission to distribute their manuscript
through the repository system.
C8'