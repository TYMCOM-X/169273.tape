$margin 12
$width 90
$spacing 2
$title 'An Optimizing Pascal Compiler'
1.\\\&Introduction&
$skip $need 3 $para 5
It is commonly assumed that it ought to be easy to generate efficient code
for structured programs written in a structured language.
Wirth proposed the development of implementations "which are both
reliable and efficient on presently available computers" as a principal
aim of the language Pascal [13].
However, the very advantages of high-level, structured languages such as Pascal
present unique challenges to the compiler writer.
For example, the structure of Pascal programs introduces opportunities for
interprocedural optimizations which are not available in compiling a language
such as Fortran.
Similarly, techniques can be devised to improve the optimization of
programs using structured data.
$skip $need 3 $para 5
We have designed and implemented a production optimizing compiler for Pascal
on the DECsystem-10.
Our compiler incorporates algorithms from a variety of sources.
We have adapted these algorithms to optimize Pascal more effectively,
and have modified them to work together.
This paper discusses several of these algorithms as they are used in our
compiler.
$skip $need 3 $para 5
Our compiler accepts the full Pascal language,
including features posing particular problems for optimization, such
as goto statements out of procedures, and
procedural and functional parameters.
Our language also provides certain common extensions.
For example, we allow separate compilation of "modules", which may
share procedures, functions, and variables;
and we permit functions to return values of any type.
All of these standard and extended language features are accomodated by our
optimization algorithms.
$skip $need 3 $para 5
We have found that certain language changes which have been recommended
elsewhere  result in
disproportionate improvements in the optimization algorithms.
In particular, we forbid assignments to value parameters, and we require that
the parameterization of functional and procedural parameters be fully
specified.
$skip $need 3 $para 5
The structure of our compiler differs from that of most Pascal compilers,
which have fundamentally the same one-pass, recursive descent structure as
Wirth's CDC Pascal compiler [14].
The program representation used in our optimizer is novel,
as it assumes two different forms during optimization.
These forms are not related to the PCODE representation produced by the
Portable Pascal compiler [1], on which other multi-pass compilers
have been based.
Our original intermediate form is machine-independent, and manipulates data
represented as records, arrays, sets, etc.
This is converted to a low-level form which more closely models the actual
memory accesses and computations required on the target machine.
Our intermediate form allows a great deal of flexibility for optimization;
we apply optimizing transformations to both the original and low-level
intermediate forms.
$skip $need 3 $para 5
The code generated by our compiler is 30-40% smaller than that
produced by the DECsystem-10 compiler written by Nagel et al.\[8],
which is representative of many one-pass Pascal compilers.
It should be noted that this reduction was achieved without attempting to
recognize many special-case optimization
opportunities (e.g., assignments of the form "X\:=\X\op\Y"),
and with a primitive register allocation algorithm.
We intend to add such optimizations in the future.
$skip $need 3 $para 5
Our compiler runs in three passes.
The parser reads the program source code and produces a
symbol table and a high-level intermediate form representation.
The optimizer transforms the initial, high-level intermediate form (HLIF)
into optimized low-level intermediate form (LLIF).
The code generator converts the low-level intermediate form into machine code.
In this paper, we consider only the optimizing pass of the compiler.
$skip 2 $need 6
2.\\\&Optimizer Architecture&
$skip $just $para 5
Both the high-level and the low-level intermediate forms are composed of
n-tuples,
which are divided into statement tuples and expression tuples.
All tuples have explicit pointers to the expression tuples representing
their operands,
so expressions may be processed recursively as tree structures.
In addition, all the tuples in the intermediate form are linked together in
a linear list, in which a tuple follows all of its operands,
so the intermediate form may be processed in a simple linear scan.
This hybrid form simplifies the recovery of tuples which become unreferenced
during optimization
and the computation of usage counts for register allocation [7].
$skip $need 3 $para 5
The optimization pass begins with application of the interprocedural analysis
algorithm (section 3).
It then applies the following steps to each procedure in the program (including
the main program itself):
$indent +5 $skip $need 3 $para -5
(a)\\Flow graph analysis.
Basic blocks are recognized, jumps to jumps are eliminated, and boolean
operators are converted to branch logic.
If there are procedure or function calls which might result in non-local
gotos to labels in this procedure, special flow graph edges are introduced
from the calls to the labels, using information obtained during interprocedural
analysis.
The basic blocks of the intermediate form are reordered, using the depth-first
spanning tree (DFST) ordering on the flow graph [2],
which allows easy detection of loops and improves the efficency of a number
of the algorithms which are applied later.
$skip $need 3 $para -5
(b)\\High-level optimization.
The basic blocks of the program are reduced by elimination of common
subexpressions, and global
(intraprocedural) optimizations are applied.
$skip $need 3 $para -5
(c)\\Shaping.
This step converts the high-level intermediate form into the low-level
intermediate form.
The HLIF and LLIF use the same statement tuples, and most of the same
expression operator tuples.
They differ in their treatment of data references.
In the HLIF, there are tuples for references to constants, variable identifiers,
array elements, fields of records, etc.
In the LLIF, these are replaced by tuples representing immediate values, memory
addresses, and the contents of memory words in terms of constant or symbolic
offsets and computed base and index operands.
The general form of the LLIF is machine-independent, but the transformations
in this step may be tailored to the addressing structure of the target machine.
The LLIF is simplified by application of simple algebraic rules.
For example, the constant part of a subscript calculation may be replaced
by a constant offset in an indexed memory reference.
$skip $need 3 $para -5
(d)\\Low-level optimization.
The shaping step frequently introduces redundant computations, particularly
in the code generated to compute the addresses of array elements.
These redundant computations are eliminated by the low-level optimization step.
$indent -5 $skip $need 3 $para 5
In this paper, we will only consider the algorithms used in interprocedural
analysis and in high-level and low-level optimization,
since the flow graph analysis is largely conventional, and the shaping step
is more closely related to code generation than to optimization.
$skip 2 $need 6
3.\\\&Interprocedural Analysis&
$skip $para 5
Interprocedural analysis actually comprises two distinct tasks,
quick block analysis and summary data flow analysis.
$skip $need 3 $para 5
Two language features make interprocedural analysis particularly interesting.
The first is the ability to define a routine (i.e., a procedure or function)
or a variable as PUBLIC, meaning
that it may be referred to in separately compiled programs, or as EXTERNAL,
meaning that its actual definition is in a separately compiled program.
The second is the ability to specify procedural and functional parameters and
variables.
We require that all such parameters and
variables be declared with a "routine type",
which fully specifies their parameterization.
$skip $need 3 $para 5
Interprocedural analysis makes use of the program call graph.
The call graph contains nodes for the main program, all external routines,
all routine variables, each defined routine, and each procedural or
functional parameter.
Edges in the call graph represent calling and binding relationships.
We assume that a call on a routine with a procedural or functional parameter
will result in a call on its parameter (an assumption which is both conservative
and reasonable), and define the edges in the call graph to reflect this
assumption.
$skip 2 $need 6
3.1\\Quick Block Analysis
$skip $para 5
The performance of a highly modularized, block structured program is strongly
affected by the procedure linkage protocol.
The need to initialize stack frames and to maintain a static chain or display
imposes an overhead on all routine calls.
Quick block analysis determines whether a routine can share the stack frame
of a caller (the routine's "owner").
Since a quick routine uses a portion of the stack frame of its owner for
its local variables, it does not need to allocate and initialize a
stack frame of its own, and can be invoked with an abbreviated calling
sequence.
In our case, the call, entry, and return code for quick routines requires only
two instructions,
as compared to nine instructions for normal routines.
$skip $need 3 $para 5
As a result of quick block analysis,
a local routine (that is, a routine which is declared within another
routine) often shares the stack frame of the routine within which it is
declared.
In this case, there is no need to trace a static chain or use a display when
the inner routine refers to variables declared in the outer routine.
Thus, quick block analysis reduces the effective depth of routine nesting
in a program.
$skip $need 3 $para 5
Recognition of quick routines is based on the following observations:
$indent +5 $skip $need 3 $para -5
(1)\\The main program and any public routines are non-quick by definition,
since they are entry points of the entire program, and must have their own
stack frames.
$skip $need 3 $para -5
(2)\\A recursive routine is non-quick, since it must create a new stack frame
for each instantiation of its local variables.
$skip $need 3 $para -5
(3)\\A routine which is called by two other routines with different owners
is non-quick, since it must have a unique owner.
$skip $need 3 $para -5
(4)\\A routine which is passed as a parameter or assigned to a variable
is non-quick, since the set of its possible callers is unknown.
$indent -5 $skip $need 3
Any other routine must be called only by routines having a common owner,
and may share the stack frame of that common owner.
A non-quick routine is its own owner.
Note that there is no need for a routine to be declared within its owner.
$skip $need 3 $para 5
We have developed linear algorithms for determining the owners of all
routines in the program, and for overlaying the local storage for routines
sharing the same stack frame.
These algorithms are based on the depth-first spanning tree of the call graph,
and do not require bit-vector or bit-matrix techniques.
$skip 2 $need 6
3.2\\Summary Data Flow Analysis
$skip $para 5
Summary data flow analysis determines which variables may be used and/or
modified by a routine call.
This information is needed to deal properly with calls during
subsequent optimization.
$skip $need 3 $para 5
Our algorithm for summary data flow analysis is derived from algorithms
due to Barth [4,5] and Hecht [9].
Using cross-reference information saved by the parser, we compute, for each
routine P, the sets DIRECTMOD(P) and DIRECTUSE(P).
DIRECTMOD(P) is the set of variables which are modified by P itself, and
DIRECTUSE(P) is the set of variables whose values are used in P.
Using these sets and the call graph,
the algorithm computes MOD(P) and USE(P), the sets of all the variables which
may be modified or used as a consequence of a call on P.
$skip $need 3 $para 5
For purposes of our algorithm, the set of all variables includes all declared
variables and variable parameter symbols in the program.
For each pointer type, all the dynamically allocated variables which can be
accessed with pointers of that type are treated as a single variable.
We also treat declared labels as variables, including a label
symbol in DIRECTMOD(P) if P contains a goto statement
to that symbol.
This artifice allows us to obtain the information about non-local goto
statements which is needed for flow graph analysis.
$skip $need 3 $para 5
We make two important simplifying assumptions in our algorithm.
First, we assume that a variable parameter of a routine is modified by the
routine.
This assumption is reasonable because of our prohibition on assignments to value
parameters, which allows us to pass value parameters by reference, rather than
having to copy them.
This eliminates the copying overhead associated with call by value in
standard Pascal.
Thus, there is no motivation to declare a variable parameter
unless it will actually be modified in the procedure, and it is reasonable
to assume that any variable passed as a variable parameter is modified.
$skip $need 3 $para 5
We also specify that if a routine accesses the same
variable by more than one name, i.e., if two parameters refer to the same
actual variable, or if the routine makes a global reference to a variable
which is passed to it as a parameter, the results are undefined.
In conjunction with the first assumption above, this obviates the need for
any sort of aliasing analysis on parameters.
This simplifies the algorithm considerably.
Furthermore, if a procedure has a variable parameter which it modifies,
Barth's algorithm will indicate that any call on this procedure will
modify all variables which are ever bound to the parameter.
Our algorithm, in contrast, will only indicate a modification of the variable
actually passed in each call to the procedure.
$skip $need 3 $para 5
Let N be the number of procedures in the call graph, and E the number of edges.
The algorithm described by Barth uses bit-matrix representations of the CALLS,
DIRECTMOD, and DIRECTUSE relations, and computes the MOD and USE relations
with bit-matrix operations.
Its run time is therefore at least O(N\).
Hecht's algorithm is based on Tarjan's depth-first spanning tree algorithm
for strongly connected components [3,11], and therefore has run time O(N+E),
but it was designed for SIMPL-T, a non-block structured language in which
scope considerations do not arise.
By applying Tarjan's algorithm to nested subgraphs of the call graph at
successive levels of scope,
our algorithm computes MOD and USE as precisely as possible
under the assumption that all statements in a procedure are executable.
Since the quick block analysis described above reduces the effective depth
of nesting, the worst-case run time for our algorithm is O((N+E)*L),
where L is the maximum effective nesting level of routine definitions.
This is typically less than the true maximum depth of nesting.
$skip 2 $need 6
4.\\\&Basic Block Reduction&
$skip $para 5
The basic blocks of the program are reduced by an algorithm based on the
value number method described by Cocke and Schwartz [6].
The list of tuples for a basic block is scanned, keeping a hash-accessed table
of unique expressions, and setting a pointer in each expression tuple to the
expression tuple representing its value.
$skip $need 3 $para 5
Since Pascal programs tend to make extensive use of structured variables,
our algorithm detects cases where the value of one component of
an array or record is left unchanged by an assignment to some other component.
For example, it can detect that the value of a[i+1] is unchanged by an
assignment to a[i].
Also, by keeping track of assignments to tag fields, it can recognize cases
where the value of p^.f1 is unaffected by an assignment to q^.f2, even when
f1 and f2 are fields in different variants of the same record type.
$skip $need 3 $para 5
The reduction algorithm can also detect redundant assignments.
An assignment whose left and right sides have the same values is simply
eliminated.
An assignment whose left side is later reassigned is eliminated, but the
right side is still evaluated if it could have side effects.
$skip 2 $need 6
5.\\\&Global Optimization&
$skip $para 5
Our global optimization uses algorithms due to Reif [10], which make use of
the dominator tree of the program flow graph.
Essentially, we perform common subexpression elimination, not only within basic
blocks, but also along dominator chains.
We begin by constructing the dominator relation for the flow graph.
We then construct the sets of references whose values can be modified on some
path from the immediate dominator of each basic block to that block.
From these sets, we can find the earliest dominating block from which the
value of each reference is unchanged.
Finally, we use this information to reapply the basic block reduction
algorithm, reducing entire dominator chains instead of basic blocks.
That is, we can eliminate a tuple in block A if there is matching tuple in
block A or in any block which dominates A.
This algorithm uses a tree representation for the UNION-FIND set problem [3,12],
and has run time O((N+E) log N), where N is the number of basic blocks in the
flow graph, and E is the number of edges.
$skip $need 3 $para 5
Since the algorithms used in this process are bit-vector
algorithms, we need a mapping from the intermediate form expressions to the
bit-vector indices which
minimizes the size of the bit-vectors without losing too
much useful information about the data flow in the program.
We let each bit-vector index represent a class of actual reference expressions,
and our algorithm recognizes the same sorts of relations between
fields in related record variables that are handled by the basic block
optimizer.
$skip 2 $need 6
6.\\\&Low-level Optimization&
$skip $para 5
Like the high-level global optimization algorithm, low-level optimization
finds common subexions along dominator chains of the flow graph.
However, this algorithm makes no attempt to optimize assignments or
memory references, since optimizations involving the values and lifetimes of
variables are applied during high-level optimization.
Low-level optimization is only concerned with new expressions
which may be introduced during shaping.
Consequently, the low-level optimization algorithm is a very simplified
version of the algorithm that is used for basic block and global reduction
during high-level optimization.
$skip 2 $need 6
\\\\\&References&
$justify left $skip
1.\\\Ammann, U. The method of structured programming applied to the development
of a compiler. Proc.\ACM Intern.\Compiler Symp.\1973. North-Holland,
New York, 1973, p.\93.
$skip $need 3
2.\\\Aho, A.V., and Ullman, J.D. &Principles of Compiler Design&.
Addison-Wesley, Reading, Mass., 1977.
$skip $need 3
3.\\\Aho, A.V., and Ullman, J.D. &The Design and Analysis of Computer
Algorithms&. Addison-Wesley, Reading, Mass., 1974.
$skip $need 3
4.\\\Barth, J.M. A practical interprocedural data-flow analysis algorithm.
&Comm.&\&ACM& 21, 9 (Sept.\1978), 724-736.
$skip $need 3
5.\\\Barth, J.M. A practical interprocedural data flow analysis algorithm and
its applications. Ph.D.\Th., U.\of California, Berkeley, 1977.
$skip $need 3
6.\\\Cocke, J., and Schwartz, J.T. &Programming Languages and their Compilers:
Preliminary Notes&. New York University, 1970.
$skip $need 3
7.\\\Freiburghouse, R.A. Register allocation via usage counts. &Comm.&\&ACM&
17, 11 (Nov.\1974), 638-642.
$skip $need 3
8.\\\Grosse-Lindemann, C.O., and Nagel, H.H. Postlude to a Pascal-compiler
bootstrap on a DECsystem-10. &Software&--&Practice and Experience& 6, 29-42 (1976).
$skip $need 3
9.\\\Hecht, M.S. &Flow Analysis of Computer Programs&. North-Holland, New York,
1977.
$skip $need 3
10.\\Reif, J.H. Combinatorial aspects of symbolic program analysis.
Ph.D.\Th., Harvard U., 1977.
$skip $need 3
11.\\Tarjan, R.E. Depth-first search and linear graph algorithms.
&SIAM J.&\&Comput.&\1, 2 (June 1972), 146-160.
$skip $need 3
12.\\Tarjan, R.E. Applications of path compression on balanced trees.
Tech.\Rep.\528, Comptr.\Sci.\Dept., Stanford U., Oct.\1975.
$skip $need 3
13.\\Wirth, N. The programming language Pascal (revised report).
Berichte der Fachgruppe Computer Wissenschaften ETH Zurich, No.\5 (1973).
$skip $need 3
14.\\Wirth, N. The design of a Pascal compiler. &Software&--&Practice and
Experience& 1, 309-333 (1971).
 	 	m—