


   Performance Analysis on the Tymcom-X Operating System






     1.0  INTRODUCTION

     The purpose of this document is to outline issues relating  to  the
     performance  of  Tymcom-X  systems.   These  issues  include  tools
     available for performance measuring, environment of Tymcom-X, known
     areas  in the operating system where performance improvements might
     be expected, and ideas for projects which may result in performance
     improvements.



     2.0  ENVIRONMENT

     2.1  Unique Operating System

     Manufacturer provided operating systems usually have  a  subset  of
     customers   located  at  universities  and  corporations  who  both
     investigate performance and write new utilities and schedulers  for
     the  manufacturer's  operating  system.   It is not unusual for PhD
     theses to be based on such efforts.  The Tymcom-X operating systems
     group   is   operating  at  a  disadvantage  with  respect  to  the
     manufacturer's operating systems groups, since  there  is  no  user
     community to share performance improvement efforts.



     2.2  Multiplicity Of Systems

     The Tymcom-X operating system currently exists in a somewhat unique
     environment.   One  unique  aspect  of this environment is that the
     operating system runs on more than 20 systems, and is supported  by
     only  one  operating  systems  group.  The performance of operating
     systems provided by a manufacturer to customer sites is not  always
     an issue, and when it is, the operating system parameters are tuned
     individually for that site  by  a  systems  analyst  based  on  the
     statistical characteristics of the load at that site.  At Tymshare,
     our loads vary widely enough so that each system would have  to  be
     tuned  individually.   This  would  require  more personnel than we
     either have or could  acquire  in  a  reasonable  amount  of  time.
     Another problem is that our loads vary frequently enough (customers
     moved from system to system, losing customers,  gaining  customers)
     that  the  tuning  would  require constant attention.  We therefore
     have to state our objectives with respect to how  many  systems  we
     should  "tune"  and  under what conditions.  Also entering into the
     situations is the fact that we run four types of  processors,  each
     with a completely different set of performance characteristics.

     Tymshare  gains  advantage  over  in-house  computing  by   pooling
                                                                  Page 2


     resources  such as individual systems analysts for each system, but
     this also limits the amount of fine tuning we can realistically do.
     Each  adjustment  to  a  parameter  that  is made must be evaluated
     seperately for all systems.  What may be a favorable change for one
     system  and  load  may  turn  out to be unfavorable on another.  To
     solve this problem generally  would  be  equivalent  to  writing  a
     scheduler which tuned itself!

     This is not to say there aren't  changes  that  would  benefit  the
     performance  on  most  systems, such as using more efficient coding
     techniques or eliminating adverse effects which are  seen  on  most
     systems.   However,  there  is  a class of performance improvements
     which have a delicate tradeoff balance which we may never be  in  a
     position  to  make on all our systems.  Setting optimal quantum run
     times is an example of  a  parameter  which  would  have  different
     effects for different system loads.

     We need  to  have  a  clear  idea  of  what  kinds  of  performance
     improvement  activities we can afford to undertake.  It seems clear
     to me that most of our efforts will have to be made in the area  of
     general   improvements,   with  occasional  efforts  made  to  tune
     individual loads which are felt to be stable ones.



     2.3  Concurrent Update Environment

     A relatively recent demand on the system which is  somewhat  unique
     is  brought  on  by  the  concurrent  update applications which are
     starting to be implemented.  The  additional  factor  is  that  the
     performance  of  the  machine  is  not  only  determined by how the
     operating  system  interacts  with  the  users,  as  in   our   old
     environment, but in how the users interact with each other.  A good
     example of this is the "club interlock" problem, which is explained
     fully later in this document.  A high demand for this interlock has
     made the interlock a critical resource which can adversely effect a
     large  number of users on the system simultaneously.  Therefore, if
     the interlock is given out to a job which is not currently in core,
     a  condition  of  very  low  throughput can result.  The higher the
     load, the more likely jobs will be swapped out and the more  likely
     the  interlock  will  be  given out to a swapped out job.  The fact
     that the operating system  gives  out  the  interlock  to  the  job
     waiting  longer actually makes it more likely that the job will not
     be in core because of the fact that the  swapper  has  very  little
     idea of who has club interlocks and who is waiting for them.



     3.0  TOOLS

                                                                  Page 3


     3.1  Existing Tools

     3.1.1  SNOOPY - SNOOPY's main function is to gather a PC  histogram
     for  user  or  exec  mode PCs.  The PC can be qualified by sampling
     only for a specific job number or program name.



     3.1.2  ICP - ICP originally started out as a program to investigate
     the  effects  of in core protect time on paging performance.  Since
     then, it has been extended to include CPU utilization, fault  rate,
     general  status  of the working set (all there or not) for each job
     selected.  It also includes a general section  which  reports  lost
     time,  microcycles  per  second, page rate per second for input and
     output, and event counts per  second  for  a  multitude  of  system
     events.   With  ICP,  it  is  possible to tell 1) If there is heavy
     paging going on or not;  2) If a certain job or jobs is  getting  a
     significantly greater portion of their share of the CPU;  3) If one
     or more jobs has a high fault rate  (for  either  user  or  monitor
     working  set);   4)  If CPU is fully utilized.  One can get a rough
     idea of  total  system  overhead  by  comparing  the  system  total
     microcycles  per second against the known average rate for the type
     of  system  being  monitored,  but  this  is  only  valid  if   the
     microcycles/sec  ICP  reports  is  much  different than the average
     (100000-150000 for KI, 200000-300000 for KL).  This is because many
     system  operations  that  are  not scheduling, swapping, or context
     switching overhead run with the microcycle  clock  turned  off  and
     increment  a TRU component instead.  For example, if a job is doing
     many INPUT UUOs from the disk, the TRU  component  for  disk  reads
     gets  incremented  and  all  the  INPUT  UUO  code  runs  with  the
     microcycle clock off.

     ICP has a mode in which it can run detached and write its data  out
     periodically  into  a  file.   This  procedure must be initiated by
     hand, and the data written out is in ICP's usual ASCII  format,  in
     which  the  fields vary widely, making it very difficult to write a
     data reduction program to summarize the data.  Another  problem  is
     that  ICP  uses enough CPU time to significantly effect the load of
     the system it is trying to measure.

     When run on the F3, ICP shows no microcycle measurement as there is
     no runtime clock available on that type of processor yet.



     3.1.3  READST - READST is  a  program  that  produces  graphs  that
     summarize  some  very  general  performance  data  that the program
     CHKPNT collects.  CHKPNT collects this data every 5  minutes.   The
     data  includes  number  of  jobs  logged in, null, lost, idle, used
     time, total paging rate, "file system" I/O rate, TRU rate,  average
     size  of  jobs  broken  down into several groups, total size of all
     jobs.  The size information is largely obsolete  because  the  size
     reported  doesn't take sharing into account, and is the job's total
     size rather than its working set size.  READST has been valuable in
     the past to find out when during a given day the load on the system
                                                                  Page 4


     is the greatest, and if there is any time during  a  day  when  the
     system becomes "overloaded".  This is determined mainly by watching
     the used time, TRU rate, lost time, and  swapping  rate.   If  lost
     time  becomes significantly greater than 5%, the system is starting
     to "page itself to death".  This is usually accompanied by a  sharp
     rise  in  swap  rate  and a drop in TRU rate.  READST is not a good
     tool for detailed performance analysis, as it only gives data  that
     is  averaged  over  5  minute  intervals,  and  doesn't report much
     detailed information about what is going on inside the system.  For
     example, it doesn't report number of TTY wakeups per second, number
     of jobs in the run  queue,  number  of  jobs  waiting  for  swapper
     service,  etc.   READST  does  not  report  data for specific jobs,
     either.  The main strength of READST is to provide  data  which  is
     constantly  gathered which can be referred to in order to determine
     if there is a load problem on a system in general.



     3.1.4  ACTSCN - This program has very limited use.  Its function is
     to  produce  a  summary  of  TRU  usage  sorted  by username over a
     specified interval of time.  Its use  in  performance  analysis  is
     mainly  to  identify  usernames  which  are getting more than their
     share of system resources over a period of time too long for ICP to
     detect.



     3.2  New Tools

     It is very likely that  either  existing  tools  will  have  to  be
     extended  or  new tools will have to be created to aid in improving
     operating system and application performance.



     3.2.1  Stretch Time Measurement - Significantly lacking  among  the
     system  tools  are  the  metrics  necessary to measure elapsed time
     waiting in the various queues to resources, and elapsed time  using
     those  resources.   With  these  numbers,  the system can compute a
     "stretch time", the factor by which the user's use of that resource
     has been slowed down by waiting.  Elapsed time using a resource can
     also be used to  indicate  where  an  application  can  reduce  its
     resource  requirements.  For example, if most of the time during an
     application's transactions is spent  doing  disk  I/O,  either  the
     application  can  be  changed  to do less disk I/O or the operating
     system's efficiency in doing disk I/O can be increased.



     3.2.2  Application Monitoring - If the operating system kept  track
     of  the  number  of operations of critical system functions that an
     application was causing, the data could be used  to  either  change
     the  application  or  the  operating system to increase efficiency.
     The operating system does keep track of some system  functions  and
     events  in  the  job's  TRU component cells.  Some other counts are
                                                                  Page 5


     missing.  For example, the number of context switches a job  causes
     for various reasons (TI wait, TO wait, IO wait, waiting for swapper
     service) is not kept track of  directly  in  the  operating  system
     currently.   Also useful would be to monitor the duration of actual
     time spent using a resource (CPU, disk I/O, PCBs,  club  interlock,
     etc.) in order to see where the most performance improvement payoff
     lies.  In the future, a new mechanism might have to be  created  to
     collect  this information for a job, since capacity expansion plans
     for the operating system currently  include  moving  even  existing
     data  into the swappable part of the job's monitor data base (known
     as the context pages).



     3.2.3  Other System Performance Measurements Needed - A  method  of
     measuring system scheduling, context switching, and core allocation
     overhead would provide useful data.  Currently, this  can  only  be
     done  by  running SNOOPY on the system to be monitored and watching
     the PC histogram.  The problem with this is  that  if  there  is  a
     specific  system  with an overhead problem, there is no way to just
     read out what the overhead is.  One has to guess that  there  is  a
     problem,  setup  a  SNOOPY  buffer  in the operating system for the
     system being watched, and run  SNOOPY  to  see  what  the  overhead
     really is.

     Amount of time spent running jobs  from  each  of  the  CPU  queues
     (Q0,1,2,3) would be useful in identifying whether the higher queues
     tended to lock out lower ones.  Also, a way to monitor quantum time
     expiration   might  be  appropriate,  since  the  quantum  time  is
     decremented for every voluntary reschedule (necessary to keep  jobs
     running  for  less  than  a  tick  from  staying in a high priority
     queue).  This information would be useful to  gather  for  specific
     jobs,  to see if they will have a tendency to stay in higher queues
     or not.  Without this sort of information adjusting  quantum  times
     in  the  scheduler  to  optimize  response or performance is random
     guesswork at best.

     Some measurement to determine the  arrival  rate  of  jobs  at  the
     various  queues  is  necessary to get an idea of how fast the queue
     population is changing.  This is especially meaningful for the CPU,
     where jobs do not occupy the resource for the entire time they need
     it.  The arrival  rate  of  jobs  being  runnable,  seperated  into
     categories  corresponding  to  their  previous state (disk I/O, TTY
     I/O, etc) would distinguish situations of relatively  low  resource
     demands  but  high  request  rates  from  relatively  high resource
     demands with low request rates.   This  information  is  useful  in
     setting  scheduler  quanta  and  in  planning  the  core allocation
     strategy (since the core allocater operates at a much  slower  rate
     than the CPU scheduler, an algorithm which associates CPU with core
     allocation priority such as we have currently can cause  confusion.
     If  it  is  determined  that  there  is a high request rate for CPU
     service but low occupation rate, we will know the swapper will  get
     into trouble.)
                                                                  Page 6


     3.2.4  Test Load - It will be  necessary  for  initial  performance
     testing  to  have a repeatable test load to use in comparing before
     and after measurements.  This load should  be  easily  maintainable
     without  much  outside  help  needed  and  should  also  be  fairly
     representative of some real application somewhere.



     4.0  POTENTIAL AREAS FOR PERFORMANCE IMPROVEMENT

     4.1  CPU Overhead

     4.1.1  Activate - It has been observed on many occasions  that  the
     process  of  activating  and deactivating pages in a job's map have
     taken between 15 and  20%  of  system  CPU  time.   Activation  and
     deactivation are two of the primary functions of the paging system.
     The main sub-functions of activation are

     1.  Finding the disk page in the core hash table if it is there

     2.  Checking  to  see  if  the  page  is  "virgin"  (there  is  the
         complication  of  shared private pages - have to look in SPT to
         see if they are still virgin)

     3.  If page is a virgin file page,  try  to  start  it  in  without
         calling swapper

     4.  Make sure core page  accounting  indicates  that  the  page  is
         available  (could  be in core, but reserved by someone else for
         another disk page)

     5.  If available, increment core page use count, changing core page
         lists if necessary

     6.  Put page onto LMA chain (easy  for  activate,  puts  it  on  in
         beginning  of  chain,  so  don't  have  to map in anyone else's
         context pages)

     7.  If the page has just become shared, there can be a single  slot
         somewhere  that  is  active  and unshared.  Because of possible
         presence of super-slots  (which  are  handled  specially,  they
         don't count as being presence for the purposes of sharing) must
         go thru LMA chain to find that slot and  change  it  to  shared
         status.   (I  don't  remember  exactly  why this is necessary -
         possibly something to look into)

     8.  Finally setup map slot with writable bit on  iff  the  page  is
         dirty, and return.


     This routine could be sped up to some extent by doing  things  such
     as   checking  the  virgin  bit  before  calling  TSTVIR,  removing
     debugging checks.
                                                                  Page 7


     4.1.2  Deactivate - This routine does the following:

     1.  Wait for direct I/O into the page to stop

     2.  Take slot out of LMA chain (easy if its the first  member,  not
         easy  if  its in the middle or end).  (LMA chain exists to link
         all map slots together so that when a page becomes  clean,  the
         system  can  turn  off  all  the  writable  bits.) (Idea - what
         percentage of private pages get dirtied  -  enough  to  justify
         always making them dirty and not putting them on LMA chain?)

     3.  If shared slot,  just  put  SPT  pointer  back  into  slot  and
    decrement the core page use count.

     4.  If not shared slot, generate deactivated format LMAP  slot  and
         store back into LMAP.  Also zero PGYATB if not a super slot.




     4.1.3  Disk Page Allocation - CPU  spikes  of  a  reasonably  small
     size, but enough to be called spikes, have been observed in the SAT
     searching routines.  Perhaps if the system kept a buffer  of  pages
     available  which  it  could  then give out quickly until exhaustion
     would reduce time spent in this area.



     4.1.4  SIMIO - "Old Style" disk I/O leads to increased  time  spent
     in  the  monitor.  Any program that directly maps in its pages will
     use less CPU time.



     4.2  Disk I/O Problems

     4.2.1  Core Allocation Cycle - Because of the design of the system,
     all  disk I/O to pages not already present in core must find a free
     core page and read the entire page into the new  core  page.   This
     slows up disk I/O because a core allocation cycle must be performed
     (except in the case of the first reference to a mapped  file  page,
     which  may  have  its  I/O  started  up  immediately if there is an
     available core page) before the page is read in, while in  the  old
     swapping type systems the I/O can be performed directly into memory
     that has already been allocated.  This has  the  effect  of  adding
     extra  CPU overhead for performing disk I/O, since on a busy system
     there will not be many clean free available pages to start the  I/O
     into,  thus, an available page must be found by deactivating a page
     from another job.
                                                                  Page 8


     4.2.2  Output Only Pages - All non-private disk  page  I/O  on  the
     system  consists  of  first reading in the existing data from disk,
     modifying it, and writing it  back  out.   This  is  true  even  in
     situations  where  the file page was just allocated and there is no
     non-zero data in the page.  The system currently cannot distinguish
     between  a  freshly  allocated  file  page and one with data in it.
     Thus, if one wants to write data into a newly allocated  file  page
     which  is  known  by  the user to be newly allocated, one output is
     required to clear the page when it is allocated, an input  required
     to  read  in  the  zeroes  when  it is mapped and written into, and
     another output is  eventually  done  when  the  user  has  finished
     writing  his  data  out.   If  an  operation was provided to take a
     private page which the user had written the new data, validate  it,
     and  insert that private page into a file, this would eliminate the
     zeroing I/O and the reading in I/O.  The  validation  is  necessary
     because the old data in the disk page must be erased somehow before
     the disk address is written into the file's RIB, or else the system
     could  crash  before  the  disk  page was zeroed and after the disk
     address got put into the RIB, allowing the user to access  the  old
     disk  data.   The MAGNUM people claim that this would speed certain
     operations in MAGNUM up considerably.  The implementation  of  this
     feature  would  probably be limited to inserting private pages into
     files, since to do so with file pages would require  a  significant
     complication  to  the shared disk page data base.  The problem with
     doing this for file pages is that an  arbitrarily  long  string  of
     pointers  must  be  created as the page gets moved from one file to
     another, and this would make the design inherently  unreliable  and
     more  inefficient  due to more code to scan and check the inter-SPT
     chains.  If the operation was limited to private  pages,  the  move
     could  be  made only once, thus the chaining level would be limited
     to one level.  The inter-SPT  link  is  necessary  because  of  the
     following  situation:   a  shared  page may have some inactive LMAP
     slots which point to the old SPT.  It is not possible to find these
     slots  until  they  are  activated  or  removed.   Since  an SPT is
     associated either with a specific file or a job's private pages  as
     a  group,  when  a  page whose entry appears in a given SPT changes
     from file to file or from private pages to a file,  all  the  slots
     have  to  be eventually pointed to the new SPT.  A mechanism has to
     be created to have the old SPT entry point to the new SPT entry, so
     that  when  the other users of the page remove the page, the remove
     code doesn't mistakenly turn  the  "mapped"  bit  off  in  the  SAT
     because  of the SPT use count in the old SPT getting decremented to
     zero.  (The sole reason for an SPT's existence is to know  when  to
     turn the "mapped" bit back off, and the reason for the "mapped" bit
     is so that file page deallocation doesn't have to check a  list  of
     currently  mapped  file  pages  before allowing the disk page to be
     deallocated from the file.)
                                                                  Page 9


     4.2.3  Working Set Paging Vs. "File" I/O - On  the   old   swapping
     systems,  a disk drive was allocated which was not made part of the
     file  structure  to  handle  swapping.   This  kept  swapping  from
     interfering  with  actual  disk I/O by perturbing the heads of only
     one drive with swapping activity, and while the swapping drive  was
     seeking,  any of the other drives on the system could be doing disk
     I/O.  In the paging system, all drives are subject  to  both  "disk
     I/O"  and  "swapping",  which  causes "disk I/O" rates to drop as a
     result of the far more numerous "swapping" I/O requests in the disk
     queues.   ("disk I/O" here is defined as reading in a file page for
     the first time after it has been mapped.) One solution to  this  is
     to  give the disk I/O (at the expense of apparently high throughput
     and swap response time) higher priority.  Another  solution  is  to
     allocate  private  pages  on  a seperate disk unit.  This would not
     prevent pages that were mapped from  files  from  interfering  with
     "real  disk  I/O" when they were swapped in and out, but if private
     pages were allocated from a single  disk  unit  one  could  keep  a
     "swapping  sat" around in core and not have to get and release PCBs
     to allocate  and  deallocate  private  pages.   This  is  a  fairly
     extensive  modification  to  the  system,  however,  and  it is not
     certain that the performance increase  would  justify  complicating
     the  system  in this way.  I would recommend the priority scheme be
     tried first, since it is safe relative to the private  page  scheme
     and would include file pages as well as private.



     4.2.4  Write Page Faults - Because the TOPS10 style  paging  design
     doesn't have any "shared pointers", each virtual page map slot that
     points to a clean page must  have  the  writable  bit  turned  off.
     Therefore,  when  a page goes from dirty to clean status, all slots
     that have the writable bit turned on must be changed  to  turn  the
     writable bit back off.  Also, overhead in the page fault routine is
     generated whenever a clean page is written into, since a page  trap
     must  happen,  finally resulting in the dirty flag in the core page
     data base to get set.  This code could be sped up in the case where
     no  one  has the context page interlock by bypassing the usual page
     fail code if the Access bit for the page is on, the  fault  is  for
     writing, and the writable bit is off.



     4.3  Working Set Algorithm

     The working set algorithm was found to take up 10% of a 2020  given
     the  original  interval of page aging.  This interval was increased
     considerably to alleviate this problem for the 2020, and  was  also
     increased  for  the  other  machines,  on  which  the  overhead was
     proportionally smaller due to their speed advantage over the  2020.
     Re-evaluating  this  algorithm  may  result in a way to reduce this
     overhead  without  adversely  affecting  the  fault  rate.    Also,
     optimizing  the  page  fault code path for page aging should reduce
     the overhead in page fault code.

     In general, the less often the working set is recomputed the higher
                                                                 Page 10


     the fault rate will be and the lower the recomputation overhead is.
     This results in a tradeoff of unknown proportions between  spending
     CPU  time and disk I/O faulting, and spending CPU time in computing
     the working set more accurately.



     4.4  Club Interlock Problem

     MAGNUM uses the interlock of a club to interlock the procedure of a
     user  getting a MAGNUM lock for a page.  A problem arises when many
     users are trying to get this lock at once.  Eventually, a situation
     can  arise where the lock is given to a user who has MRQ set.  This
     forces all other jobs that are waiting for the interlock,  some  of
     which  are  ready to run, to wait for that one job's working set to
     be brought in by the swapper, which can be done at  a  rate  of  60
     pages  per  second  on  the average.  This means that a 60 page job
     with MRQ set and none of  its  pages  in  core  can  be  given  the
     interlock  and increase the waiting time by a full second.  If this
     happens enough, jobs which have been in IL state for  a  long  time
     can become more likely to be swapped out, and so it can become more
     likely to give interlock away to swapped out job, and everyone gets
     stuck  in  IL  state  with  little  work being done.  This has been
     observed on the COMARS system -  the  interlock  being  given  away
     about  once  per  second and over 50% of the time to a job that had
     MRQ set.

     A general solution to this would have to be one in  which  the  job
     that had been waiting the longest would be started in off the disk,
     and while that job was on its way in the lock would be  given  away
     to waiters who were ready to run.  This assumes the in-core waiters
     do not hold the lock for long, since if they did the job we brought
     in may get kicked back out again before it gets the lock.  There is
     a problem with the current implementation of the swapper and  clubs
     which  prevents  this  solution from being implemented easily.  The
     swapper determines which jobs to bring in  according  to  the  same
     queues  the  scheduler  uses.   The  only time the club lock can be
     given to a job is if the job is in the  IL  scheduler  queue.   The
     only  way  to force the swapper to bring in a job more quickly than
     normal is to move it from the IL queue  to  the  run  queue.   This
     results  in  erasing  the only indicator the system had for knowing
     whether or not the job is in a state to receive the club lock.

     Another possible solution would be to give the lock away  to  those
     in core the majority of the time, and give it to the longest waiter
     for a minority of the time.



     5.0  IDEAS FOR IMPROVING PERFORMANCE

                                                                 Page 11


     5.1  Random Access File Performance

     Because almost all  MAGNUM  database  access  is  random,  the  DDB
     pointer areas which are designed to optimize sequential file access
     are not very effective.  Thus, every time MAGNUM  maps  in  a  page
     from  a file, chances are that the RIB as to be read in to find out
     which disk page the relative file page belongs to.



     5.1.1  Contiguously Allocated File - If  there  were   a   way   to
     preallocate  a  contiguous file on the disk, no RIB access would be
     necessary to relate relative page in the file to actual disk  page.
     One  would  only  have  to add the relative page number to the base
     disk address of the file.  This would speed up access accordingly.



     5.1.2  Better DDB Pointer Structure - If the cache of disk pointers
     obtained  from the RIB were not strictly sequential copies from the
     RIB, but had a relative file page number associated with them,  and
     there  was  a  mechanism  to keep the most frequently used pointers
     around longer, the result might be less RIB access.   Since  MAGNUM
     files  are  tree  structured,  the higher pages tend to be accessed
     more frequently than lower  pages  in  the  tree  structure.   This
     change  has  a high risk factor, since it means re-writing parts of
     the file system that are critical to file integrity.



     5.1.3  Using Frame As Source Of Frequently Used Pages - If a single
     frame  (probably  only worth it for shared data bases) were to have
     the most frequently used  pages  from  relations  mapped  into  its
     address  space, replicates from that frame could be used instead of
     MAPs.  This would bypass having to convert relative  file  page  to
     disk address.



     5.2  Reformat Of Disk

     Currently the disk is formatted in 576 byte records  without  keys,
     which comes to 85 pages per cylinder.  There are 808 cylinders used
     (remaining ones are alternate cylinders), resulting in 68680  pages
     per  pack.  The maximum number of pages that can be transferred per
     second is 270 (at 3600 rpm).

     If the disk were reformated in pages, 5 pages per  track  could  be
     accomodated,  resulting  in  95 pages per cylinder, 76760 pages per
     disk unit, about 12% increase in disk capacity.  At 3600  rpm,  300
     pages per second would be the maximum transfer rate, an increase of
     11.1% over the current maximum rate.  (These transfer rates do  not
     include seek times.)

     Currently the maximum transfer rate we have seen  is  approximately
                                                                 Page 12


     120  pages  per second.  If one assumes the number of seeks will be
     decreased by the same percentage as  the  capacity  increase  (best
     case)  a  total  transfer  rate  increase  of  11%  would  be seen.
     Assuming the same number of seeks per second (worst case) we see  a
     current  minimum  transfer time of 8.3 ms.  Assuming 3.7 ms of this
     is transfer time, this gives 4.6 ms seek time  per  transfer  (less
     than  the minimum specified time because of overlapped seeks).  3.3
     ms + 4.6 ms = 7.9 ms, a 5% increase over  8.3  ms.   Therefore  the
     increase in transfer rate should be between 5 and 11%.



     5.3  Scheduler

     5.3.1  Scheduler Parameter Tuning - Data about the  typical  events
     that  happen  during  a  MAGNUM transaction would have to be either
     measured or guessed at to change  and  tune  scheduler  parameters.
     Much  work has been done in the past to find out what effect if any
     changing in-core protect times had on  swapping  performance.   The
     results  seemed  to simply change overhead time for lost time on an
     even basis.  Changing quantum run times can have two  effects:   1)
     changing  context  switching rates, thus overhead;  2) changing the
     way the processor is allocated over several types of jobs.  No more
     throughput can be achieved by changing quantum times to be smaller.
     The only effect smaller  quantum  times  will  have  is  to  reduce
     response  time  for  jobs  whose  transactions  take  less  time to
     complete than  other  jobs  with  greater  transaction  time.   The
     optimum  value  for quantum times depends very much on the specific
     characteristics of the application load, and therefore have  to  be
     handled on a case by case basis.  If "stretch time" were measurable
     on a job by job basis, the  quantum  times  could  be  adjusted  to
     optimize the times for the jobs desired.

     Also note that shortening quantum time has the  effect  of  causing
     CPU  priorites  to vary more quickly.  Since the swapper's priority
     is the same as the CPU priority, and the swapper acts  more  slowly
     then  the CPU can switch between jobs, this can result in some less
     than optimal decisions by the swapper as to which  job  to  service
     next,  since  by the time it gets a job that had high priority into
     core, many more can have even greater priority.  This  problem  was
     partly alleviated by limiting the number of "big swaps" the swapper
     did to one at a time.  Previous to this  change  the  swapper  kept
     adding  to  the swap in list until the number of pages that were on
     their way in exceeded a certain number.  Trying to limit the number
     of  "big  swaps"  by  making  that  number  small  also  caused  an
     impediment to "real" disk I/O,  and  making  it  large  caused  the
     swapper  to  do  too  much  swapping  of  "big jobs", some of whose
     priorites were relatively low by the time  they  had  been  brought
     into core.

     It is also important to note  that  different  scheduler  parameter
     values  will  be  optimal for each MAGNUM application, so tuning of
     these parameters will have to  take  place  on  an  application  by
     application basis (see Environment section).
                                                                 Page 13


     5.3.2  Disk Scheduling - It has been mentioned  above  under  known
     deficiencies  that swapping I/O can flood out "real" disk I/O.  See
     that section for further details.



     5.3.3  Externally Settable Priority Scheduler - The   ability    to
     associate  priorities  among jobs on a system doesn't seem to solve
     the problem of how to run 60+ transaction processing MAGNUM jobs on
     a given system all of which have the same priority.  A possible use
     for a capability like this would be to select some  non-transaction
     activities,   such  as  report  generation,  and  make  them  lower
     priority.  However, any scheduler fancier than the current one will
     take  up  more  CPU time, and this has to be considered against any
     possible gain.  It is more efficient to run  these  lower  priority
     jobs after hours, if that is possible.



     6.0  SUMMARY

     My opinion is that the places where we have the most to gain are 1)
     decreasing  CPU  overhead  to  obtain  more  processing  power;  2)
     Maximizing utilization of disk I/O  capacity  by  eliminating  disk
     transfers  and prioritizing them to favor "real work" as opposed to
     "swapping".  I am very doubtful as to whether or not we  will  ever
     be able to give acceptable response times to large numbers of users
     on a machine which cannot fit almost all  active  working  sets  in
     core,  since  it  takes  almost a full second to bring in a 60 page
     working set (which is approximately the size of the non-shared part
     of a MAGNUM job's working set.) It is therefore very important that
     we  put  enough  memory  on  our  machines  to   accomodate   these
     situations.
 8  wg