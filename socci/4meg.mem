


     1.0  INTRODUCTION

     Recently the following question came up:  "If we had 4 megawords of
     memory,  what  could  we  do with it?" This is a different question
     than how much memory do we need on the -10s.   I  got  to  thinking
     about  what  it  would be like if so much memory was available that
     not all of it was needed for active  frame's  working  sets.   This
     little paper contains the idea I came up with.

     Essentially, the particular type of application we want to optimize
     is  the "typical" MAGNUM data base application.  It has always been
     my opinion that  to  make  response  time  smaller  and  throughput
     larger,  the key for MAGNUM will be to make accessing the data base
     pages faster and more efficient.  This could include both ideas  to
     reduce  CPU  overhead  required  to access a page and to reduce the
     average number of pages which are accessed for each data base item.

     One way to reduce the number of disk accesses needed to  get  to  a
     MAGNUM  data  page is to allow MAGNUM to access an area of the disk
     directly, without having to access RIBs to find out where the pages
     are.   This  should  be done in a way that protects the majority of
     the file system from MAGNUM file system problems, i.e.  only  allow
     MAGNUM  to write in certain limited areas of the disk, or reserve a
     disk unit, or maybe certain areas of each  disk  unit,  for  MAGNUM
     data  base  pages.   After  this  is done, MAGNUM still may have to
     access more than one file page to get to a data page because of its
     own index pages.



     2.0  THE IDEA

     If more than enough memory was  available,  some  number  of  pages
     could be reserved so that the data in them was not reclaimed by the
     swap out scan.   The  application  could  specify  which  pages  it
     thought  were  important enough to warrant this treatment and which
     weren't.   Even  though  all  frames  have  removed  one  of  these
     important  pages, its data would remain in memory until replaced by
     another "important" page.  Since in the paging system  memory  acts
     as  a  cache  for  the disk, this would mean that less I/O would be
     necessary to access data pages  of  the  data  base.   The  optimum
     situation would be a large data base all of whose index pages could
     fit in the number of pages marked  to  be  reserved  for  important
     pages.   Then, the only I/O needed would be to bring in the desired
     data page.

     This mechanism could be taken a step further.  The PCB system could
     operate  from this reserved page pool.  Then, the application could
     specify when the file was looked up or entered whether or  not  the
     file  is  a frequently accessed file.  If it is, then its RIB would
     be brought into a page marked as "important".

     Essentially, this scheme divides core pages up  into  two  classes.
     The "important" class of page would only be taken out of memory for
     another "important" page.  The replacement would probably be  first
                                                                  Page 2


     come  first  serve.   The non-important pages would be replaced the
     way pages are now.



     3.0  IMPLEMENTATION

     When a page is mapped, a new bit would be used to say that the page
     is  important.   (A  system  counter  should be kept to monitor the
     number of times that more important pages are  declared  than  have
     been reserved for the system).  When this page is referenced, it is
     seen to be "important".  If it is already  used,  the  system  just
     makes sure the core page is marked as important.  If the page needs
     to be brought in, a normal core allocation cycle is done to  get  a
     page  to  bring  the data into.  (The important bit will have to be
     part of the  LMAP  slot).   The  number  of  "important"  pages  is
     incremented  when  a new page is used to bring "important" data in.
     If the limit of important pages has now been exceeded, the swap out
     scan  may  re-use the number of important pages that exist over the
     system limit.  For  example,  suppose  there  is  a  limit  of  300
     important  pages  in  the  system,  and  there  are  currently  305
     important pages existing.  The swap out scan can re-use as many  as
     5  core  pages  marked  as important, according to its regular core
     page replacement algorithm (which is currently  clean  free,  clean
     available  in progress, dirty free.  see SWPRS2 in SCHED1, which is
     part of the core page reserving routine SWPRES).



     4.0  INTERACTION WITH PCB SYSTEM

     As mentioned earlier, it would be nice if the PCB system  could  be
     integrated  back  into  the  main paging system.  The PCB system is
     currently a miniature paging system,  with  a  few  extra  features
     added.   One  extra  feature is the lock system (completely locked,
     write locked, read locked, etc.).  Another feature  is  the  "write
     again"  feature, which automatically makes sure that if a core page
     is written into while its being written onto the disk, the transfer
     to  the  disk  will  be started up again after the end of the first
     transfer to disk.   This  is  necessary  because  the  file  system
     depends  on  all  data  written into a PCB to be started out to the
     disk as soon as the PCB is released by the last user.  Then, WATPCB
     can  be  called later, and the user can be sure all changes made to
     his file will have gotten to the disk.  Yet another feature is  the
     special  treatment  of SAT PCBs, which get treated differently than
     RIB PCBs with respect to when  they  get  written  out.   All  this
     special  behavior  will  have  to  be  considered so that it is all
     duplicated when converting PCBs to the regular paging system.   Not
     only functionality, but performance will have to be considered.  It
     would not be acceptable for the  file  system  overhead  to  go  up
     considerably.

     For example, in the current PCB system the  user  doesn't  have  to
     wait  until  the  data in the PCB being released is actually out on
     the disk to continue with his processing.  In fact, the  next  user
                                                                  Page 3


     of that PCB, if he wants a different disk page in the PCB, may have
     to wait for the IO from the previous user to finish before starting
     his  disk  data into that PCB.  In the regular paging system, there
     is currently no way to remove a page (which is  the  equivalent  of
     calling  MAPRLS  for  PCBs) without waiting for that page's data to
     get out to disk, since  VREMOV  does  a  VALPAG  with  wait  before
     returning.   Even if it just did a VALPAG without wait, there would
     still be a need to later see if that page had  made  it  out.   The
     current  method  of waiting for all PCBs with I/O in progress to go
     out will not work as well in the normal paging system, since  there
     are  many  more  pages  in the normal paging system than in the PCB
     system.  To get the same performance, new functions will have to be
     added.   Also,  a  question that comes to my mind is whether or not
     the logic in VREMOV to make sure your data gets  out  to  the  disk
     even  though  disk  output is in progress for the page is efficient
     enough compared to the current write again logic in the PCB system.
     There  are probably other performance and functional considerations
     that must be dealt with that I haven't thought  of  which  must  be
     handled  to  replace  the  PCB  mini-paging system with the regular
     paging system.

     Also note that the equivalent of the "important" page  system  must
     be  done before the PCB system can be changed to use normal paging.
     The reason for this is that the current PCB paging  system  uses  a
     certain  number  of  fixed pages that are guaranteed to exist.  The
     file system never has to wait for a core page to start file  system
     disk  data  in.   In  order  to  approximate  current  file  system
     performance, the "important" page system has to be  implemented  to
     make sure that file system disk data doesn't get thrown out of core
     quickly.  Furthermore, a quick method of reclaiming core pages that
     have  been  used  for the file system has to be implemented so that
     the user doing a LOOKUP doesn't have to wait for a core  allocation
     to  happen  before  reading  in the RIB for a file.  The problem is
     that there might not be a free core page to grab  immediately.   It
     might  be  necessary  to reserve a certain number of core pages for
     use by the file system.  It might be sufficient to just always make
     file  system  pages  "important", and have a limit on the number of
     these that exist in the system.  When the  limit  is  exceeded,  an
     important  page  being that was used by the file system gets reused
     for the new file system request.  I haven't really thought all this
     out  as  far  as  it needs to be, but these are the kinds of things
     that have to be considered.

     A word of warning:  I  once  implemented  a  mechanism  called  "IO
     reserved  pages",  which  attempted  to reserve a certain number of
     clean free pages into which disk data for I/O type page  references
     (virgin  file  pages)  could could be started quickly.  I never got
     the mechanism to work because of a page accounting type bug that  I
     never  found  the  solution  to.   I  don't remember what the exact
     problem was, but anyone who tries to implement a  reserve  pool  of
     clean available pages will probably run into the problem too.
                                                                  Page 4


     5.0  PRIORITY OF DISK I/O

     It would probably help file system efficiency  if  disk  I/O  could
     also  be split up into at least two classes.  The first class could
     be for file system and user file data requests  (virgin  file  page
     references).   The second class could be for I/O needed to bring in
     frames' working sets.  This prioritization would have the effect of
     limiting  working  set  thrashing if there was a significant demand
     for I/O for the file system and user file data.  Almost all  paging
     system   swapper   improvements  have  had  the  characteristic  of
     inherently reducing the number and kind of working sets that we try
     to  bring  in for a certain period of time.  When we introduced the
     rule that only one frame which had been  eated  from  which  needed
     more  than 10 pages brought in could be handled at a time (BIGFIT),
     it allowed many more  smaller  requests  to  be  handled  at  once,
     resulting  in  better  throughput for actual data transfer requests
     and inherently limiting the amount of working set swapping  we  did
     at  a time.  The swapper's CPU overhead is directly proportional to
     the number of pages it deals with in a swapper  call,  so  reducing
     the number of large working sets we deal with at a time reduced CPU
     overhead.
